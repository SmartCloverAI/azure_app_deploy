# This file has been autogenerated by version 1.57.0 of the Azure Automated Machine Learning SDK.


import numpy
import numpy as np
import pandas as pd
import pickle
import argparse


# For information on AzureML packages: https://docs.microsoft.com/en-us/python/api/?view=azure-ml-py
from azureml.training.tabular._diagnostics import logging_utilities


def setup_instrumentation(automl_run_id):
    import logging
    import sys

    from azureml.core import Run
    from azureml.telemetry import INSTRUMENTATION_KEY, get_telemetry_log_handler
    from azureml.telemetry._telemetry_formatter import ExceptionFormatter

    logger = logging.getLogger("azureml.training.tabular")

    try:
        logger.setLevel(logging.INFO)

        # Add logging to STDOUT
        stdout_handler = logging.StreamHandler(sys.stdout)
        logger.addHandler(stdout_handler)

        # Add telemetry logging with formatter to strip identifying info
        telemetry_handler = get_telemetry_log_handler(
            instrumentation_key=INSTRUMENTATION_KEY, component_name="azureml.training.tabular"
        )
        telemetry_handler.setFormatter(ExceptionFormatter())
        logger.addHandler(telemetry_handler)

        # Attach run IDs to logging info for correlation if running inside AzureML
        try:
            run = Run.get_context()
            return logging.LoggerAdapter(logger, extra={
                "properties": {
                    "codegen_run_id": run.id,
                    "automl_run_id": automl_run_id
                }
            })
        except Exception:
            pass
    except Exception:
        pass

    return logger


automl_run_id = 'shy_garden_wy1h3n478w_50'
logger = setup_instrumentation(automl_run_id)


def split_dataset(X, y, weights, split_ratio, should_stratify):
    '''
    Splits the dataset into a training and testing set.

    Splits the dataset using the given split ratio. The default ratio given is 0.25 but can be
    changed in the main function. If should_stratify is true the data will be split in a stratified
    way, meaning that each new set will have the same distribution of the target value as the
    original dataset. should_stratify is true for a classification run, false otherwise.
    '''
    from sklearn.model_selection import train_test_split

    random_state = 42
    if should_stratify:
        stratify = y
    else:
        stratify = None

    if weights is not None:
        X_train, X_test, y_train, y_test, weights_train, weights_test = train_test_split(
            X, y, weights, stratify=stratify, test_size=split_ratio, random_state=random_state
        )
    else:
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, stratify=stratify, test_size=split_ratio, random_state=random_state
        )
        weights_train, weights_test = None, None

    return (X_train, y_train, weights_train), (X_test, y_test, weights_test)


def get_training_dataset(dataset_uri):
    
    from azureml.core.run import Run
    from azureml.data.abstract_dataset import AbstractDataset
    
    logger.info("Running get_training_dataset")
    ws = Run.get_context().experiment.workspace
    dataset = AbstractDataset._load(dataset_uri, ws)
    return dataset.to_pandas_dataframe()


def prepare_data(dataframe):
    '''
    Prepares data for training.
    
    Cleans the data, splits out the feature and sample weight columns and prepares the data for use in training.
    This function can vary depending on the type of dataset and the experiment task type: classification,
    regression, or time-series forecasting.
    '''
    
    from azureml.training.tabular.preprocessing import data_cleaning
    
    logger.info("Running prepare_data")
    label_column_name = 'CANTITATE'
    
    # extract the features, target and sample weight arrays
    y = dataframe[label_column_name].values
    X = dataframe.drop([label_column_name], axis=1)
    sample_weights = None
    X, y, sample_weights = data_cleaning._remove_nan_rows_in_X_y(X, y, sample_weights,
     is_timeseries=True, target_column=label_column_name)
    
    return X, y, sample_weights


def generate_data_transformation_config():
    from azureml.training.tabular.featurization._featurization_config import FeaturizationConfig
    from azureml.training.tabular.featurization.timeseries.category_binarizer import CategoryBinarizer
    from azureml.training.tabular.featurization.timeseries.drop_columns import DropColumns
    from azureml.training.tabular.featurization.timeseries.grain_index_featurizer import GrainIndexFeaturizer
    from azureml.training.tabular.featurization.timeseries.missingdummies_transformer import MissingDummiesTransformer
    from azureml.training.tabular.featurization.timeseries.numericalize_transformer import NumericalizeTransformer
    from azureml.training.tabular.featurization.timeseries.restore_dtypes_transformer import RestoreDtypesTransformer
    from azureml.training.tabular.featurization.timeseries.short_grain_dropper import ShortGrainDropper
    from azureml.training.tabular.featurization.timeseries.time_index_featurizer import TimeIndexFeaturizer
    from azureml.training.tabular.featurization.timeseries.time_series_imputer import TimeSeriesImputer
    from azureml.training.tabular.featurization.timeseries.timeseries_transformer import TimeSeriesPipelineType
    from azureml.training.tabular.featurization.timeseries.timeseries_transformer import TimeSeriesTransformer
    from azureml.training.tabular.featurization.timeseries.unique_target_grain_dropper import UniqueTargetGrainDropper
    from collections import OrderedDict
    from numpy import dtype
    from numpy import nan
    from pandas.core.indexes.base import Index
    from sklearn.pipeline import Pipeline
    
    transformer_list = []
    transformer1 = DropColumns(
        drop_columns=['ID_CATEGORIE']
    )
    transformer_list.append(('drop_irrelevant_columns', transformer1))
    
    transformer2 = UniqueTargetGrainDropper(
        cv_step_size=4,
        max_horizon=4,
        n_cross_validations=5,
        target_lags=[0],
        target_rolling_window_size=0
    )
    transformer_list.append(('unique_target_grain_dropper', transformer2))
    
    transformer3 = MissingDummiesTransformer(
        numerical_columns=['SAPTAMANA_W53', 'PROC_TVA', 'SAPTAMANA_W53_W56', 'PRET_UNITAR', 'PROCENT_DISCOUNT', 'SAPTAMANA_W53_W64', 'STOC_W1', 'STOC_W2', 'STOC_M1', 'SAPTAMANA_W1', 'SAPTAMANA_W1_W52', 'SAPTAMANA_W1_W12', 'SAPTAMANA_W1_W4', 'SAPTAMANA_W53_W104']
    )
    transformer_list.append(('make_numeric_na_dummies', transformer3))
    
    transformer4 = TimeSeriesImputer(
        end=None,
        freq='W-MON',
        impute_by_horizon=False,
        input_column=['SAPTAMANA_W53', 'PROC_TVA', 'SAPTAMANA_W53_W56', 'PRET_UNITAR', 'PROCENT_DISCOUNT', 'SAPTAMANA_W53_W64', 'STOC_W1', 'STOC_W2', 'STOC_M1', 'SAPTAMANA_W1', 'SAPTAMANA_W1_W52', 'SAPTAMANA_W1_W12', 'SAPTAMANA_W1_W4', 'SAPTAMANA_W53_W104'],
        limit=None,
        limit_direction='forward',
        method=OrderedDict([('ffill', [])]),
        option='fillna',
        order=None,
        origin=None,
        value={'SAPTAMANA_W53': 1226.0, 'PROC_TVA': '0', 'SAPTAMANA_W53_W56': 4818.0, 'PRET_UNITAR': 27.65, 'PROCENT_DISCOUNT': 0.0, 'SAPTAMANA_W53_W64': 12363.0, 'STOC_W1': 298.0, 'STOC_W2': 297.0, 'STOC_M1': 288.0, 'SAPTAMANA_W1': 1660.0, 'SAPTAMANA_W1_W52': 75892.0, 'SAPTAMANA_W1_W12': 19712.0, 'SAPTAMANA_W1_W4': 6631.0, 'SAPTAMANA_W53_W104': 14726.5, 'DENUMIRE_PRODUCATOR': 'PRODUCATORNULL'}
    )
    transformer_list.append(('impute_na_numeric_datetime', transformer4))
    
    transformer5 = ShortGrainDropper(
        cv_step_size=4,
        max_horizon=4,
        n_cross_validations=5,
        target_lags=[0],
        target_rolling_window_size=0
    )
    transformer_list.append(('grain_dropper', transformer5))
    
    transformer6 = RestoreDtypesTransformer(
        dtypes={'SAPTAMANA_W53': dtype('int64'), 'SAPTAMANA_W1_W52': dtype('int64'), 'PROC_TVA': dtype('int64'), 'SAPTAMANA_W53_W56': dtype('int64'), 'PRET_UNITAR': dtype('float64'), 'PROCENT_DISCOUNT': dtype('int64'), 'SAPTAMANA_W53_W64': dtype('int64'), 'STOC_W1': dtype('int64'), 'STOC_W2': dtype('int64'), 'STOC_M1': dtype('int64'), 'SAPTAMANA_W1': dtype('int64'), '_automl_target_col': dtype('int64'), 'SAPTAMANA_W1_W12': dtype('int64'), 'SAPTAMANA_W1_W4': dtype('int64'), 'SAPTAMANA_W53_W104': dtype('int64')},
        target_column='_automl_target_col'
    )
    transformer_list.append(('restore_dtypes_transform', transformer6))
    
    transformer7 = GrainIndexFeaturizer(
        categories_by_grain_cols=None,
        grain_feature_prefix='grain',
        overwrite_columns=True,
        prefix_sep='_',
        ts_frequency='W-MON'
    )
    transformer_list.append(('make_grain_features', transformer7))
    
    transformer8 = NumericalizeTransformer(
        categories_by_col={'DENUMIRE_PRODUCATOR': Index(['PRODUCATOR1', 'PRODUCATOR10', 'PRODUCATOR11', 'PRODUCATOR12', 'PRODUCATOR13', 'PRODUCATOR14', 'PRODUCATOR15',
               'PRODUCATOR2', 'PRODUCATOR3', 'PRODUCATOR4', 'PRODUCATOR5', 'PRODUCATOR6', 'PRODUCATOR7', 'PRODUCATOR8',
               'PRODUCATOR9'],
              dtype='object'), 'grain_ID_ARTICOL': Index(['1', '10', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '11', '110', '111', '112',
               '113', '114', '115', '116', '117', '12', '13', '14', '15', '16', '17', '18', '19', '2', '20', '21', '22', '23',
               '24', '25', '26', '27', '28', '29', '3', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '4', '40',
               '41', '42', '43', '44', '45', '46', '47', '48', '49', '5', '50', '51', '52', '53', '54', '55', '56', '57', '58',
               '59', '6', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '7', '70', '71', '77', '8', '86', '87',
               '88', '89', '9', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99'],
              dtype='object'), 'grain_ID_FIRMA': Index(['10', '11', '114', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26',
               '27', '28', '29', '30', '53', '55', '59', '60', '79', '80', '85', '86'],
              dtype='object')},
        exclude_columns={'SAPTAMANA_W1_W52', 'SAPTAMANA_W53_W56', 'SAPTAMANA_W1', 'PRET_UNITAR', 'SAPTAMANA_W53_W104', 'SAPTAMANA_W1_W12', 'STOC_M1', 'SAPTAMANA_W1_W4', 'STOC_W2', 'SAPTAMANA_W53_W64', 'STOC_W1', 'PROCENT_DISCOUNT', 'SAPTAMANA_W53', 'PROC_TVA'},
        include_columns={'DENUMIRE_PRODUCATOR'}
    )
    transformer_list.append(('make_categoricals_numeric', transformer8))
    
    transformer9 = TimeIndexFeaturizer(
        correlation_cutoff=0.99,
        country_or_region=None,
        datetime_columns=None,
        force_feature_list=None,
        freq='W-MON',
        holiday_end_time=None,
        holiday_start_time=None,
        overwrite_columns=True,
        prune_features=True
    )
    transformer_list.append(('make_time_index_featuers', transformer9))
    
    transformer10 = CategoryBinarizer(
        columns=[],
        drop_first=False,
        dummy_na=False,
        encode_all_categoricals=False,
        prefix=None,
        prefix_sep='_'
    )
    transformer_list.append(('make_categoricals_onehot', transformer10))
    
    pipeline = Pipeline(steps=transformer_list)
    tst = TimeSeriesTransformer(
        country_or_region=None,
        drop_column_names=['ID_CATEGORIE'],
        featurization_config=FeaturizationConfig(
            blocked_transformers=None,
            column_purposes={'ID_ARTICOL': 'Categorical', 'ID_CATEGORIE': 'Categorical', 'ID_FIRMA': 'Categorical', 'START_DATE_WEEK': 'DateTime', 'PRET_UNITAR': 'Numeric', 'DENUMIRE_PRODUCATOR': 'Categorical', 'PROC_TVA': 'Numeric', 'PROCENT_DISCOUNT': 'Numeric', 'SAPTAMANA_W1': 'Numeric', 'SAPTAMANA_W1_W4': 'Numeric', 'SAPTAMANA_W1_W12': 'Numeric', 'SAPTAMANA_W1_W52': 'Numeric', 'SAPTAMANA_W53': 'Numeric', 'SAPTAMANA_W53_W56': 'Numeric', 'SAPTAMANA_W53_W64': 'Numeric', 'SAPTAMANA_W53_W104': 'Numeric', 'STOC_W1': 'Numeric', 'STOC_W2': 'Numeric', 'STOC_M1': 'Numeric'},
            dataset_language=None,
            prediction_transform_type=None,
            transformer_params={'Imputer': [[['DENUMIRE_PRODUCATOR'], {'strategy': 'constant', 'fill_value': 'PRODUCATORNULL'}], [['PROC_TVA'], {'strategy': 'constant', 'fill_value': '0'}]]}
        ),
        force_time_index_features=None,
        freq='W-MON',
        grain_column_names=['ID_ARTICOL', 'ID_FIRMA'],
        group=None,
        lookback_features_removed=False,
        max_horizon=4,
        origin_time_colname='origin',
        pipeline=pipeline,
        pipeline_type=TimeSeriesPipelineType.FULL,
        seasonality=4,
        time_column_name='START_DATE_WEEK',
        time_index_non_holiday_features=['_automl_year', '_automl_half', '_automl_quarter', '_automl_month', '_automl_day', '_automl_qday'],
        use_stl=None
    )
    
    return tst
    
    
def generate_preprocessor_config():
    '''
    Specifies a preprocessing step to be done after featurization in the final scikit-learn pipeline.
    
    Normally, this preprocessing step only consists of data standardization/normalization that is
    accomplished with sklearn.preprocessing. Automated ML only specifies a preprocessing step for
    non-ensemble classification and regression models.
    '''
    from sklearn.preprocessing import StandardScaler
    
    preproc = StandardScaler(
        copy=True,
        with_mean=False,
        with_std=False
    )
    
    return preproc
    
    
def generate_algorithm_config():
    '''
    Specifies the actual algorithm and hyperparameters for training the model.
    
    It is the last stage of the final scikit-learn pipeline. For ensemble models, generate_preprocessor_config_N()
    (if needed) and generate_algorithm_config_N() are defined for each learner in the ensemble model,
    where N represents the placement of each learner in the ensemble model's list. For stack ensemble
    models, the meta learner generate_algorithm_config_meta() is defined.
    '''
    from xgboost.sklearn import XGBRegressor
    
    algorithm = XGBRegressor(
        base_score=0.5,
        booster='gbtree',
        colsample_bylevel=1,
        colsample_bynode=1,
        colsample_bytree=1,
        enable_categorical=False,
        eta=0.1,
        gamma=0,
        gpu_id=-1,
        importance_type=None,
        interaction_constraints='',
        learning_rate=0.100000001,
        max_delta_step=0,
        max_depth=7,
        max_leaves=3,
        min_child_weight=1,
        missing=numpy.nan,
        monotone_constraints='()',
        n_estimators=50,
        n_jobs=0,
        num_parallel_tree=1,
        objective='reg:squarederror',
        predictor='auto',
        random_state=0,
        reg_alpha=2.291666666666667,
        reg_lambda=1.9791666666666667,
        scale_pos_weight=1,
        subsample=0.5,
        tree_method='auto',
        validate_parameters=1,
        verbose=-10,
        verbosity=0
    )
    
    return algorithm
    
    
def build_model_pipeline():
    '''
    Defines the scikit-learn pipeline steps.
    
    For time-series forecasting models, the scikit-learn pipeline is wrapped in a ForecastingPipelineWrapper,
    which has some additional logic needed to properly handle time-series data depending on the applied algorithm.
    '''
    from azureml.training.tabular.models.forecasting_pipeline_wrapper import ForecastingPipelineWrapper
    from sklearn.pipeline import Pipeline
    
    logger.info("Running build_model_pipeline")
    pipeline = Pipeline(
        steps=[
            ('tst', generate_data_transformation_config()),
            ('model', generate_algorithm_config())
        ]
    )
    forecast_pipeline_wrapper = ForecastingPipelineWrapper(pipeline, stddev=[72.70800314901592])
    
    return forecast_pipeline_wrapper


def train_model(X, y, sample_weights=None, transformer=None):
    '''
    Calls the fit() method to train the model.
    
    The return value is the model fitted/trained on the input data.
    '''
    
    logger.info("Running train_model")
    model_pipeline = build_model_pipeline()
    
    model = model_pipeline.fit(X, y)
    return model


def calculate_metrics(model, X, y, sample_weights, X_test, y_test, cv_splits=None):
    '''
    Calculates the metrics that can be used to evaluate the model's performance.
    
    Metrics calculated vary depending on the experiment type. Classification, regression and time-series
    forecasting jobs each have their own set of metrics that are calculated.'''
    
    from azureml.training.tabular.preprocessing._dataset_binning import get_dataset_bins
    from azureml.training.tabular.score.scoring import score_forecasting
    from azureml.training.tabular.score.scoring import score_regression
    
    y_pred, _ = model.forecast(X_test)
    y_min = np.min(y)
    y_max = np.max(y)
    y_std = np.std(y)
    
    bin_info = get_dataset_bins(cv_splits, X, None, y)
    regression_metrics_names, forecasting_metrics_names = get_metrics_names()
    metrics = score_regression(
        y_test, y_pred, regression_metrics_names, y_max, y_min, y_std, sample_weights, bin_info)
    
    try:
        horizons = X_test['horizon_origin'].values
    except Exception:
        # If no horizon is present we are doing a basic forecast.
        # The model's error estimation will be based on the overall
        # stddev of the errors, multiplied by a factor of the horizon.
        horizons = np.repeat(None, y_pred.shape[0])
    
    featurization_step = generate_data_transformation_config()
    grain_column_names = featurization_step.grain_column_names
    time_column_name = featurization_step.time_column_name
    
    forecasting_metrics = score_forecasting(
        y_test, y_pred, forecasting_metrics_names, horizons, y_max, y_min, y_std, sample_weights, bin_info,
        X_test, X, y, grain_column_names, time_column_name)
    metrics.update(forecasting_metrics)
    return metrics


def get_metrics_names():
    
    regression_metrics_names = [
        'root_mean_squared_log_error',
        'mean_absolute_percentage_error',
        'residuals',
        'predicted_true',
        'spearman_correlation',
        'explained_variance',
        'mean_absolute_error',
        'root_mean_squared_error',
        'median_absolute_error',
        'r2_score',
    ]
    forecasting_metrics_names = [
        'forecast_residuals',
        'forecast_table',
        'forecast_mean_absolute_percentage_error',
        'forecast_adjustment_residuals',
    ]
    return regression_metrics_names, forecasting_metrics_names


def get_metrics_log_methods():
    
    metrics_log_methods = {
        'root_mean_squared_log_error': 'log',
        'forecast_table': 'Skip',
        'mean_absolute_percentage_error': 'log',
        'residuals': 'log_residuals',
        'forecast_mean_absolute_percentage_error': 'Skip',
        'forecast_adjustment_residuals': 'Skip',
        'predicted_true': 'log_predictions',
        'spearman_correlation': 'log',
        'explained_variance': 'log',
        'mean_absolute_error': 'log',
        'root_mean_squared_error': 'log',
        'forecast_residuals': 'Skip',
        'median_absolute_error': 'log',
        'r2_score': 'log',
    }
    return metrics_log_methods


def main(training_dataset_uri=None):
    '''
    Runs all functions defined above.
    '''
    
    from azureml.automl.core.inference import inference
    from azureml.core.run import Run
    from azureml.training.tabular.score._cv_splits import _CVSplits
    from azureml.training.tabular.score.scoring import aggregate_scores
    
    import mlflow
    
    # The following code is for when running this code as part of an AzureML script run.
    run = Run.get_context()
    
    df = get_training_dataset(training_dataset_uri)
    X, y, sample_weights = prepare_data(df)
    tst = generate_data_transformation_config()
    tst.fit(X, y)
    ts_param_dict = tst.parameters
    short_series_dropper = next((step for key, step in tst.pipeline.steps if key == 'grain_dropper'), None)
    if short_series_dropper is not None and short_series_dropper.has_short_grains_in_train and grains is not None and len(grains) > 0:
        # Preprocess X so that it will not contain the short grains.
        dfs = []
        X['_automl_target_col'] = y
        for grain, df in X.groupby(grains):
            if grain in short_series_processor.grains_to_keep:
                dfs.append(df)
        X = pd.concat(dfs)
        y = X.pop('_automl_target_col').values
        del dfs
    cv_splits = _CVSplits(X, y, frac_valid=None, CV=5, n_step=4, is_time_series=True, task='regression', timeseries_param_dict=ts_param_dict)
    scores = []
    for X_train, y_train, sample_weights_train, X_valid, y_valid, sample_weights_valid in cv_splits.apply_CV_splits(X, y, sample_weights):
        partially_fitted_model = train_model(X_train, y_train, transformer=tst)
        metrics = calculate_metrics(partially_fitted_model, X, y, sample_weights, X_test=X_valid, y_test=y_valid, cv_splits=cv_splits)
        scores.append(metrics)
        print(metrics)
    model = train_model(X_train, y_train, transformer=tst)
    
    metrics = aggregate_scores(scores)
    metrics_log_methods = get_metrics_log_methods()
    print(metrics)
    for metric in metrics:
        if metrics_log_methods[metric] == 'None':
            logger.warning("Unsupported non-scalar metric {}. Will not log.".format(metric))
        elif metrics_log_methods[metric] == 'Skip':
            pass # Forecasting non-scalar metrics and unsupported classification metrics are not logged
        else:
            getattr(run, metrics_log_methods[metric])(metric, metrics[metric])
    cd = inference.get_conda_deps_as_dict(True)
    
    # Saving ML model to outputs/.
    signature = mlflow.models.signature.infer_signature(X, y)
    mlflow.sklearn.log_model(
        sk_model=model,
        artifact_path='outputs/',
        conda_env=cd,
        signature=signature,
        serialization_format=mlflow.sklearn.SERIALIZATION_FORMAT_PICKLE)
    
    run.upload_folder('outputs/', 'outputs/')


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--training_dataset_uri', type=str, default='azureml://locations/westeurope/workspaces/def84eb2-d667-4eb4-8f9a-70505c89c4e3/data/synthetic_weekly_product_sale_train/versions/1',     help='Default training dataset uri is populated from the parent run')
    args = parser.parse_args()
    
    try:
        main(args.training_dataset_uri)
    except Exception as e:
        logging_utilities.log_traceback(e, logger)
        raise